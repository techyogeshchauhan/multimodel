{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pypdf2 pillow transformers sentence-transformers torch scikit-learn pymupdf pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from pyngrok import ngrok\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (1.25.1)\n",
      "Requirement already satisfied: pillow in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: transformers in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: torch in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: tqdm in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: scipy in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cair/miniconda3/envs/multimodel/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymupdf pillow transformers sentence-transformers torch scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import display, Image as IPImage\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class MultimodalRAG:\n",
    "    def __init__(self):\n",
    "        # Check if GPU is available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        print(\"Initializing models...\")\n",
    "        # Move models to GPU\n",
    "        self.text_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        self.text_model.to(self.device)\n",
    "        \n",
    "        self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.image_model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.image_model.to(self.device)\n",
    "        \n",
    "        self.chat_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "        self.chat_model.to(self.device)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "        \n",
    "        self.text_chunks = []\n",
    "        self.text_embeddings = []\n",
    "        self.images = []\n",
    "        self.image_embeddings = []\n",
    "        self.image_locations = []\n",
    "        self.page_text_map = {}\n",
    "        print(\"Initialization complete!\")\n",
    "        \n",
    "    def chunk_text(self, text, chunk_size=500, overlap=100):\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            chunk = ' '.join(words[start:start + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def process_text(self, text, page_num):\n",
    "        if not text.strip():\n",
    "            return\n",
    "            \n",
    "        if page_num not in self.page_text_map:\n",
    "            self.page_text_map[page_num] = []\n",
    "            \n",
    "        chunks = self.chunk_text(text)\n",
    "        for chunk in chunks:\n",
    "            if len(chunk.strip()) > 50:\n",
    "                self.text_chunks.append(chunk)\n",
    "                self.page_text_map[page_num].append(chunk)\n",
    "                # Move input to GPU for encoding\n",
    "                embedding = self.text_model.encode(chunk, convert_to_tensor=True)\n",
    "                # Move back to CPU for storage\n",
    "                embedding = embedding.cpu().numpy()\n",
    "                self.text_embeddings.append(embedding)\n",
    "        \n",
    "    def extract_content_from_pdf(self, pdf_path):\n",
    "        print(f\"Processing PDF: {pdf_path}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        \n",
    "        for page_num in tqdm(range(total_pages), desc=\"Processing pages\"):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "            # Extract text\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for block in blocks:\n",
    "                text = block[4]\n",
    "                self.process_text(text, page_num)\n",
    "            \n",
    "            # Extract images\n",
    "            image_list = page.get_images(full=True)\n",
    "            for img_index, img in enumerate(image_list):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                    \n",
    "                    image = Image.open(io.BytesIO(image_bytes))\n",
    "                    if image.mode == 'RGBA':\n",
    "                        image = image.convert('RGB')\n",
    "                    \n",
    "                    # Move input to GPU\n",
    "                    inputs = self.image_processor(images=image, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        image_features = self.image_model.get_image_features(**inputs)\n",
    "                    \n",
    "                    self.images.append(image)\n",
    "                    # Move back to CPU for storage\n",
    "                    self.image_embeddings.append(image_features.cpu().numpy())\n",
    "                    self.image_locations.append((page_num, img_index))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_index} on page {page_num}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        if self.text_embeddings:\n",
    "            self.text_embeddings = np.vstack(self.text_embeddings)\n",
    "            \n",
    "        print(f\"Processing complete! Found {len(self.text_chunks)} text chunks and {len(self.images)} images.\")\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        results = {\n",
    "            'text': [],\n",
    "            'images': [],\n",
    "            'text_locations': [],\n",
    "            'image_locations': [],\n",
    "            'context': []\n",
    "        }\n",
    "        \n",
    "        if len(self.text_chunks) > 0:\n",
    "            # Move query to GPU for encoding\n",
    "            text_query_embedding = self.text_model.encode(query, convert_to_tensor=True)\n",
    "            text_query_embedding = text_query_embedding.cpu().numpy()\n",
    "            \n",
    "            text_similarities = cosine_similarity(\n",
    "                [text_query_embedding],\n",
    "                self.text_embeddings\n",
    "            )[0]\n",
    "            \n",
    "            top_text_indices = np.argsort(text_similarities)[-top_k:][::-1]\n",
    "            results['text'] = [self.text_chunks[i] for i in top_text_indices]\n",
    "            results['text_locations'] = top_text_indices\n",
    "            \n",
    "            for idx in top_text_indices:\n",
    "                page_num = None\n",
    "                for p, chunks in self.page_text_map.items():\n",
    "                    if self.text_chunks[idx] in chunks:\n",
    "                        page_num = p\n",
    "                        break\n",
    "                        \n",
    "                if page_num is not None:\n",
    "                    context = \"\\n\".join(self.page_text_map[page_num])\n",
    "                    results['context'].append((context, page_num))\n",
    "        \n",
    "        if len(self.images) > 0:\n",
    "            # Move query to GPU for processing\n",
    "            image_query_inputs = self.image_processor(text=[query], return_tensors=\"pt\", padding=True)\n",
    "            image_query_inputs = {k: v.to(self.device) for k, v in image_query_inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_query_features = self.image_model.get_text_features(**image_query_inputs)\n",
    "            \n",
    "            # Move back to CPU for similarity calculation\n",
    "            image_query_features = image_query_features.cpu().numpy()\n",
    "            \n",
    "            image_similarities = cosine_similarity(\n",
    "                image_query_features,\n",
    "                np.vstack(self.image_embeddings)\n",
    "            )[0]\n",
    "            \n",
    "            top_image_indices = np.argsort(image_similarities)[-top_k:][::-1]\n",
    "            results['images'] = [self.images[i] for i in top_image_indices]\n",
    "            results['image_locations'] = [self.image_locations[i] for i in top_image_indices]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def generate_response(self, query, context):\n",
    "        prompt = f\"\"\"Based on the following context, answer the question.\n",
    "        \n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        # Correctly handle tokenizer inputs\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.chat_model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=512,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response.split(\"Answer: \")[-1].strip()\n",
    "\n",
    "    def chat(self, query):\n",
    "        \"\"\"Single method to handle the entire chat process\"\"\"\n",
    "        print(f\"\\nQuestion: {query}\\n\")\n",
    "        \n",
    "        # Search for relevant content\n",
    "        results = self.search(query)\n",
    "        \n",
    "        # Combine context from relevant text chunks\n",
    "        context = \"\"\n",
    "        if results['context']:\n",
    "            context = \"\\n\".join([ctx[0] for ctx in results['context']])\n",
    "        \n",
    "        # Generate and display response\n",
    "        response = self.generate_response(query, context)\n",
    "        print(f\"Answer: {response}\\n\")\n",
    "        \n",
    "        # Display relevant images if any\n",
    "        if results['images']:\n",
    "            print(\"Relevant images:\")\n",
    "            for i, (img, loc) in enumerate(zip(results['images'], results['image_locations']), 1):\n",
    "                print(f\"Image from page {loc[0] + 1}:\")\n",
    "                display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 307.94 MiB is free. Process 23814 has 236.00 MiB memory in use. Including non-PyTorch memory, this process has 10.79 GiB memory in use. Of the allocated memory 10.64 GiB is allocated by PyTorch, and 45.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Process a PDF (replace with your PDF path)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/cair/Downloads/multimodel/Marma Therapy The Healing Power of Ayurvedic Vital Point Massage.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mMultimodalRAG.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_chunks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/multimodel/lib/python3.10/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multimodel/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multimodel/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multimodel/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multimodel/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 307.94 MiB is free. Process 23814 has 236.00 MiB memory in use. Including non-PyTorch memory, this process has 10.79 GiB memory in use. Of the allocated memory 10.64 GiB is allocated by PyTorch, and 45.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "rag = MultimodalRAG()\n",
    "\n",
    "# Process a PDF (replace with your PDF path)\n",
    "pdf_path = \"/home/cair/Downloads/multimodel/Marma Therapy The Healing Power of Ayurvedic Vital Point Massage.pdf\"\n",
    "rag.extract_content_from_pdf(pdf_path)\n",
    "\n",
    "# Chat with the system\n",
    "rag.chat(\"The wrist Marma – Manibandha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.chat(\"The elbow Marma – Kurpara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.chat(\"How to find Indrabasti on the arm:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.chat(\"DISTRIBUTION OF PRANIC ENERGY THROUGH NADIS TO MARMAS\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
